{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnement initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sparkF\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fdbc530a978>\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder \n",
    "        .master('spark://spark-master:7077') \n",
    "        .appName('~ TUTORIAL: Handling spark dataframe ~') \n",
    "        .config('spark.driver.cores', '1')\n",
    "        .config('spark.executor.instances', '2')\n",
    "        .config('spark.executor.memory', '4g')\n",
    "        .config('spark.executor.cores', '2')\n",
    "        .config('spark.cores.max', '4')\n",
    "        .config('spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT',1)\\\n",
    "        .config('spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT',1) \\\n",
    "        .getOrCreate())\n",
    "##- Enabling Apache Arrow\n",
    "spark.conf.set('spark.sql.execution.arrow.enabled', 'true')\n",
    "\n",
    "##- SQL context\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<b>1.  Initialize and handle your spark dataframe</b>](#Initialize-and-handle-your-spark-dataframe)\n",
    "- [Select column containing point](#Select-column-containing-point)\n",
    "<br>\n",
    "\n",
    "[<b>2.  Transform your spark dataframe</b>](#Transform-your-spark-dataframe)\n",
    "- [Standardize a spark dataframe](#Standardize-a-spark-dataframe)\n",
    "- [Compute the difference between two columns](#Compute-the-difference-between-two-columns)\n",
    "<br>\n",
    "\n",
    "[<b>3.  Operations on your spark dataframe</b>](#Operations-on-your-spark-dataframe)\n",
    "- [Count the number of missing values by column](#Count-the-number-of-missing-values-by-column)\n",
    "- [Aggregate with known function using groupby](#Aggregate-with-known-function-using-groupby)\n",
    "- [Aggregate with your own function using groupby](#Aggregate-with-your-own-function-using-groupby)\n",
    "- [Use multiple arguments with pandas UDF](#Use-multiple-arguments-with-pandas-UDF)\n",
    "- [Compute mean and standard deviation on a column](#Compute-mean-and-standard-deviation-on-a-column)\n",
    "<br>\n",
    "\n",
    "[<b>4.  Spark ML on your spark dataframe</b>](#Spark-ML-on-your-spark-dataframe)\n",
    "- [Correlation for spark DF](#Correlation-for-spark-DF)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Initialize and handle your spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select column containing point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark backtick issue : https://stackoverflow.com/questions/51253271/dropping-a-column-name-that-has-a-period-in-spark-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  a|     1|     0|\n",
      "|  a|    -1|    42|\n",
      "|  b|     3|    -1|\n",
      "|  b|    10|    -2|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "|key|value.1|value.2|\n",
      "+---+-------+-------+\n",
      "|  a|      1|      0|\n",
      "|  a|     -1|     42|\n",
      "|  b|      3|     -1|\n",
      "|  b|     10|     -2|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('value1', 'value.1').withColumnRenamed('value2', 'value.2')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##- It will print an error: it is normal !!!\n",
    "# df.select('value.1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|value.1|\n",
      "+-------+\n",
      "|      1|\n",
      "|     -1|\n",
      "|      3|\n",
      "|     10|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##- To select your column, you need to use backtick when your column name contain the character '.'\n",
    "df.select('`value.1`').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Transform your spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize a spark dataframe (HOWTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Scaling :\n",
      "+------+----+-------+----------+\n",
      "|userID|Name|Revenue|No_of_Days|\n",
      "+------+----+-------+----------+\n",
      "|     1|   A|  12560|        45|\n",
      "|     1|   B|  42560|        90|\n",
      "|     1|   C|  31285|       120|\n",
      "|     1|   D|  10345|       150|\n",
      "+------+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([ (1, 'A',12560,45),\n",
    "                             (1, 'B',42560,90),\n",
    "                             (1, 'C',31285,120),\n",
    "                             (1, 'D',10345,150)\n",
    "                           ], [\"userID\", \"Name\",\"Revenue\",\"No_of_Days\"])\n",
    "\n",
    "print(\"Before Scaling :\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UDF for converting column type from vector to double type\n",
    "# unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "# # Iterating over columns to be scaled\n",
    "# for i in [\"Revenue\",\"No_of_Days\"]:\n",
    "#     # VectorAssembler Transformation - Converting column to vector type\n",
    "#     assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "#     # MinMaxScaler Transformation\n",
    "#     scaler = StandardScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\", withStd=True, withMean=True)\n",
    "\n",
    "#     # Pipeline of VectorAssembler and MinMaxScaler\n",
    "#     pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "#     # Fitting pipeline on dataframe\n",
    "#     df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "# print(\"After Scaling :\")\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why results with StandardScaler are not the same as if I compute (value-mean)/std on my own ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of revenue is: 24187.5\n",
      "Standard deviation of revenue is: 13367.281183920686\n",
      "\n",
      " Scaled values are: \n",
      "-0.8698477902886157\n",
      "-1.0355508954693755\n"
     ]
    }
   ],
   "source": [
    "revenue = [12560,42560,31285,10345]\n",
    "print(\"Mean of revenue is: \" + str(np.mean(revenue)))\n",
    "print(\"Standard deviation of revenue is: \" + str(np.std(revenue)))\n",
    "\n",
    "print(\"\\n Scaled values are: \")\n",
    "print((12560-24187.5)/13367.281183920686)\n",
    "print((10345-24187.5)/13367.281183920686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link> https://stackoverflow.com/questions/51753088/standardscaler-in-spark-not-working-as-expected </link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the difference between two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+\n",
      "|key|value1|value2|diff|\n",
      "+---+------+------+----+\n",
      "|  a|     1|     0|   1|\n",
      "|  a|    -1|    42| -43|\n",
      "|  b|     3|    -1|   4|\n",
      "|  b|    10|    -2|  12|\n",
      "+---+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "df.withColumn('diff', col('value1')-col('value2')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Operations on your spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of missing values by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+----------+\n",
      "|userID|Name|Revenue|No_of_Days|\n",
      "+------+----+-------+----------+\n",
      "|     1|   A|  12560|        45|\n",
      "|     1|   B|  42560|        90|\n",
      "|     1|   C|  31285|       120|\n",
      "|     1|   D|  10345|       150|\n",
      "+------+----+-------+----------+\n",
      "\n",
      "+------+----+-------+----------+\n",
      "|userID|Name|Revenue|No_of_Days|\n",
      "+------+----+-------+----------+\n",
      "|     0|   0|      0|         0|\n",
      "+------+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df = spark.createDataFrame([ (1, 'A',12560,45),\n",
    "                             (1, 'B',42560,90),\n",
    "                             (1, 'C',31285,120),\n",
    "                             (1, 'D',10345,150)\n",
    "                           ], [\"userID\", \"Name\",\"Revenue\",\"No_of_Days\"])\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+----------+\n",
      "|userID|Name|Revenue|No_of_Days|\n",
      "+------+----+-------+----------+\n",
      "|     1|   A|  12560|        45|\n",
      "|     1|   B|  42560|        90|\n",
      "|     1|   C|  31285|       120|\n",
      "|     1|   D|  10345|       150|\n",
      "+------+----+-------+----------+\n",
      "\n",
      "+------+----+-------+----------+\n",
      "|userID|Name|Revenue|No_of_Days|\n",
      "+------+----+-------+----------+\n",
      "|     0|   0|      0|         0|\n",
      "+------+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df = spark.createDataFrame([ (1, 'A',12560,45),\n",
    "                             (1, 'B',42560,90),\n",
    "                             (1, 'C',31285,120),\n",
    "                             (1, 'D',10345,150)\n",
    "                           ], [\"userID\", \"Name\",\"Revenue\",\"No_of_Days\"])\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with known function using groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|key|count(value1)|\n",
      "+---+-------------+\n",
      "|  b|            2|\n",
      "|  a|            2|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "df.groupby('key').agg(count('value1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with your own function using groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1] + [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+-------+\n",
      "|key|avg_value1|avg_value2|sum_avg|sub_avg|\n",
      "+---+----------+----------+-------+-------+\n",
      "|  b|       6.5|      -1.5|    5.0|    8.0|\n",
      "|  a|       0.0|      21.0|   21.0|  -21.0|\n",
      "+---+----------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"avg_value1\", DoubleType()),\n",
    "    StructField(\"avg_value2\", DoubleType()),\n",
    "    StructField(\"sum_avg\", DoubleType()),\n",
    "    StructField(\"sub_avg\", DoubleType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "def g(df):\n",
    "    gr = df['key'].iloc[0]\n",
    "    x = df.value1.mean()\n",
    "    y = df.value2.mean()\n",
    "    w = df.value1.mean() + df.value2.mean()\n",
    "    z = df.value1.mean() - df.value2.mean()\n",
    "    return pd.DataFrame([[gr]+[x]+[y]+[w]+[z]])\n",
    "#     return pd.DataFrame([gr,x,y,w,z])\n",
    "\n",
    "df.groupby(\"key\").apply(g).show()\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  a|     2|     1|\n",
      "|  a|    -2|    43|\n",
      "|  b|     4|    -2|\n",
      "|  b|    11|    -3|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gf = spark.createDataFrame(\n",
    "[(\"a\", 2, 1), (\"a\", -2, 43), (\"b\", 4, -2), (\"b\", 11, -3)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "# gf.groupby(\"key\").apply(g).show()\n",
    "gf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multiple arguments with pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Source</b>: https://stackoverflow.com/questions/59384870/pyspark-pandas-udf-using-partial-functions-went-wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|  a|     1|     0|\n",
      "|  a|    -1|    42|\n",
      "|  b|     3|    -1|\n",
      "|  b|    10|    -2|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##- Initialize your dataframe\n",
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+-------+\n",
      "|key|avg_value1|avg_value2|sum_avg|sub_avg|\n",
      "+---+----------+----------+-------+-------+\n",
      "|  b|       6.5|      -1.5|    5.0|    8.0|\n",
      "|  a|      99.0|      21.0|   21.0|  -21.0|\n",
      "+---+----------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"avg_value1\", DoubleType()),\n",
    "    StructField(\"avg_value2\", DoubleType()),\n",
    "    StructField(\"sum_avg\", DoubleType()),\n",
    "    StructField(\"sub_avg\", DoubleType())\n",
    "])\n",
    "\n",
    "def h(threshold):\n",
    "    @pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def g(df):\n",
    "        gr = df['key'].iloc[0]\n",
    "        x = df.value1.mean()\n",
    "        y = df.value2.mean()\n",
    "        w = df.value1.mean() + df.value2.mean()\n",
    "        z = df.value1.mean() - df.value2.mean()\n",
    "              \n",
    "        if x > threshold:\n",
    "            output = pd.DataFrame([[gr]+[x]+[y]+[w]+[z]])\n",
    "        else:\n",
    "            output = pd.DataFrame([[gr]+[99]+[y]+[w]+[z]])\n",
    "        return output\n",
    "    return g\n",
    "\n",
    "df.groupby(\"key\").apply(h(2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and standard deviation on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|value1_mean|        value1_std|\n",
      "+-----------+------------------+\n",
      "|       3.25|4.7871355387816905|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as pyspark_mean, stddev as pyspark_stddev\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "df.select(pyspark_mean('value1').alias('value1_mean'),\n",
    "              pyspark_stddev('value1').alias('value1_std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Spark ML on your spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation for spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(pearson(features)=DenseMatrix(2, 2, [1.0, -0.6206, -0.6206, 1.0], False))]\n",
      "DenseMatrix([[ 1.        , -0.62056458],\n",
      "             [-0.62056458,  1.        ]])\n",
      "[ 1.         -0.62056458 -0.62056458  1.        ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "(\"key\", \"value1\", \"value2\")\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['value1', 'value2'], outputCol=\"features\")\n",
    "outdf = assembler.transform(df.select('value1', 'value2'))\n",
    "print(Correlation.corr(outdf, \"features\", \"pearson\").collect())\n",
    "corrdf = Correlation.corr(outdf, \"features\", \"pearson\")\n",
    "print(str(corrdf.head()[0]))\n",
    "\n",
    "print(Correlation.corr(outdf, \"features\", \"pearson\").collect()[0][\"pearson({})\".format(\"features\")].values)\n",
    "print(type(Correlation.corr(outdf, \"features\", \"pearson\").collect()[0][\"pearson({})\".format(\"features\")].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
