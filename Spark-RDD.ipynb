{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "   <img src=\"spark.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Hands-on: Spark RDD\n",
    "\n",
    "In this notebook you will discover the **Spark** framework and its Python API (pySpark). We will cover the basic transformations and actions with **Spark Core** and its main component: the **Resilient Distributed Dataset** (RDD). \n",
    "\n",
    "In the next notebooks, we will go throught the **SQL API** that allow higher level queries that are optimized by the Spark engine (Catalyst and Tungstene). Recall that when possible, **Spark SQL is the recommended API to analyse data** that is built on top of RDD's except you have low-level code to perform or work with very unstructured data.\n",
    "See [RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/) for a comprehensive comparison (*Dataset* API is only available in Scala language).\n",
    "\n",
    "\n",
    "Finally we will see a basic application of a machine learning algorithm that will be trained at scale through **SparkML/MLlib pipelines**.\n",
    "\n",
    "*Nota*: Spark streaming (Dstreams and Structured streaming) and Graph API are not covered in this notebook.\n",
    "\n",
    "For an introduction to the main Spark concepts, pleaser refer to the [confluence page](https://confluence.airbus.corp/display/E2H89ATAR/Introduction+to+Spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Before doing any calculation, Spark needs to be configured. To do that, you have to create a **SparkSession** that set the entry point to the master node of the cluster. With this object, you can either connect to the master node and inherit from the basic configuration of the cluster, or you can set the ressources you need for your application.\n",
    "\n",
    "*Nota*: In previous version, you had to create a SparkContext before, this is now encapsulated in SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "\n",
    "# Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Foundry\n",
    "import foundrywrapper\n",
    "from foundrywrapper import FoundryWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>\n",
       "<b>Foundry-Wrapper</b> v0.3.3</br>\n",
       "Welcome JEREMY PIRARD  (PIRARD_J)<br>\n",
       "Using <a href='https://core.skywise.com/'>core.skywise.com</a>\n",
       "</p>"
      ],
      "text/plain": [
       "Foundry-Wrapper v0.3.3\n",
       "Welcome JEREMY PIRARD (PIRARD_J)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Required for foundry, not useful for spark purposes\n",
    "fw = FoundryWrapper()\n",
    "fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession is set with the master node URL, and we overide it with a custom config. \n",
    "Here we have the driver node that holds the main application. It is dedicated at negociating ressources with the resources manager (e.g Kubernetes, YARN ...) and it is where the application is launched (typically the JAR of a spark application). We allocate 1 gigabyte of ram and 2 cpus.\n",
    "\n",
    "Then, for the workers, we use 2 executors (that are 2 JVM) with each 4 giga of RAM and 1 cores.\n",
    "\n",
    "This configuration should be enhanced with considering the computations that can be made, data size ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder \n",
    "        .master('spark://spark-master:7077') #master node URL\n",
    "        .appName('~ Spark hands-on: RDD~') #my App name\n",
    "        .config('spark.driver.memory', '1g') # memory allocated to the master\n",
    "        .config('spark.driver.cores', '2') # CPU's allocated to the master\n",
    "        .config('spark.executor.instances', '2') # how many executors\n",
    "        .config('spark.executor.memory', '4g') # memory per executor (where the data is stored)\n",
    "        .config('spark.executor.cores', '1') #CPU's per executor\n",
    "        .config('spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT',1)\\\n",
    "        .config('spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT',1) \\\n",
    "        .config(conf=fw.spark.get_spark_app_config())\n",
    "        .getOrCreate())\n",
    "# Configuring spark to use Foundry\n",
    "spark = fw.configure_spark(spark) # foundry specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the spark session, we see what version of Spark we are using (palentir3.0) and we have access to the link to the *Spark UI*. This UI is really useful to manage the workload and to see the stages of the applications running. For instance, it can be useful to monitor the ressources used and adapt it consequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.196.78.114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-palantir.65</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>~ Spark hands-on: RDD~</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c82ab8b38>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Datasets\n",
    "\n",
    "Spark has many connectors to interact with differents data format. The most common one are Parquet, JSON and csv ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset:\n",
    "\n",
    "This is the data set used for The Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections\n",
    "\n",
    "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"kddcup.data_10_percent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "f = open(data_file, 'r')\n",
    "rdd = spark.sparkContext.parallelize(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data looks like in a RDD format ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.45 ms, sys: 5.84 ms, total: 11.3 ms\n",
      "Wall time: 1.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.\\n',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.\\n',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.\\n',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.\\n',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few comments here:\n",
    "- Data is not splitted like we would do with a csv reader, with RDD you have to do it by yourself.\n",
    "- Note that the two first instructions have been processed very rapidly. This is because they are **transformations**. Recall Spark is a lazy computing engine. That means, it has done nothing before we execute an **action** with the **take** function which is intended to print out some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are two types of operations you can apply on RDD Transformations and Actions. (This is the same for dataframe)\n",
    "\n",
    "- A Transformation allows to create a new RDD from an existing one.\n",
    "- An Action applies computation on the RDD and return a value (like the .count function)\n",
    "\n",
    "*Nota:* Recall that RDD is a resilient data format. Each RDD contains somehow a link to its \"father\" that allows the cluster to recreate it in case of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "**Map** is the most common **transformation**. It applies the same function to each entries of the RDD. Let's split each record by comma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_splitted = rdd.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take\n",
    "\n",
    "**Take** is a common **action** that will print out few result lines. Be careful, calling take will execute all the transformations that lead to the result !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '181',\n",
       "  '5450',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '9',\n",
       "  '9',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.11',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.\\n'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '239',\n",
       "  '486',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '19',\n",
       "  '19',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.05',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.\\n']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_splitted.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get every feature now separated in a list with the label at the end. Let's represent it as a tuple (feature, label) while cleaning the label removing the line separator field.\n",
    "To do so let's use again map. This time the function is more complex, so we cannot use a one-liner map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0.0, 'tcp', 'http', 'SF', 181.0, 5450.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 8.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 9.0, 1.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0], 'normal'), ([0.0, 'tcp', 'http', 'SF', 239.0, 486.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 8.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 19.0, 19.0, 1.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0], 'normal')]\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction(l):\n",
    "    elems = l\n",
    "    features =[]\n",
    "    for e in  elems[:-1]:\n",
    "        try:\n",
    "            e=float(e)\n",
    "        except ValueError:\n",
    "            e=e\n",
    "        features.append(e)\n",
    "    y = elems[-1][:-2]\n",
    "    return (features, y)\n",
    "rdd_key= rdd_splitted.map(parse_interaction)\n",
    "print(rdd_key.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of normal and abnormal connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "# transformations\n",
    "normal = rdd_key.filter(lambda x: x[1] == \"normal\")\n",
    "abnormal = rdd_key.filter(lambda x: x[1] != \"normal\")\n",
    "\n",
    "#actions\n",
    "print(normal.count())\n",
    "print(abnormal.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect** is the same function as **take** except that it will retrieve the **whole** dataset to the driver! Use it with, if the data is too large it can result in a out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  181.0,\n",
       "  5450.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.0,\n",
       "  8.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.0,\n",
       "  9.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.11,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'normal')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_list = normal.collect()\n",
    "normal_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations betweens two RDD's\n",
    "\n",
    "Apply a function on RDD A et B than will produce a third RDD\n",
    "\n",
    "For instance: **subtract**. Let's create a new RDD that does contain the abnormal label using preceding RDD's. This toy example would be equivalent to a filter transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396743\n"
     ]
    }
   ],
   "source": [
    "normal_raw = rdd.filter(lambda x: \"normal.\" in x)\n",
    "abnormal_raw = rdd.subtract(normal_raw)  #would be the same as doing rdd.filter(lambda x :\"normal.\" not in x)\n",
    "\n",
    "print(abnormal_raw.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: Every transformation creates a new RDD. It is good practice to create a pipeline of transformations to have a concise coding with a clear view on the transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "This is an aggregation function that leads to a single element.\n",
    "For instance let's implement another way to count the number of normal transactions using a map/reduce style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "rdd_keyed = normal.map(lambda x : 1) # transformation: we map a 1 to every observation\n",
    "normal_count = rdd_keyed.reduce(lambda x,y :  x+y) # action\n",
    "print(normal_count) #not a rdd !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's count the total attack duration for the abnormal transaction (intrusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([184.0,\n",
       "   'tcp',\n",
       "   'telnet',\n",
       "   'SF',\n",
       "   1511.0,\n",
       "   2957.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   3.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.67,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'buffer_overflow')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2626792.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[184.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal_attack_duration = abnormal.map(lambda x : x[0][0]) #duration is the first elemet\n",
    "abnormal_attack_duration_total = abnormal_attack_duration.reduce(lambda x,y: x+y)\n",
    "print(abnormal_attack_duration_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey, reduceByKey ...\n",
    "What if we want to compute the total duration for normal connections and all the different labels referring to bad connections ?\n",
    "The **countByKey** function allows to do this for key/value RDD. There are many other actions function on these type of RDD (see the docs :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyed_rdd = rdd_splitted.map(lambda x : (x[41][:-2], float(x[0]))) # create a key value (type, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have passed in the value field whatever we wanted. For instance, we could have written (x.key, x), so now the values would be accessed by a particular key.\n",
    "The interest of doing that is to get grouped result (can be equivalent to some GroupBy in Spark SQL or pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_attack_by_key = keyed_rdd.reduceByKey(lambda x,y : x+y)\n",
    "# Here it takes a (String, Int) type and it will just groupby the keys and count them efficiently\n",
    "# If it was not grouped by key you would get an error since you can't sum such a tuple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loadmodule', 326.0),\n",
       " ('neptune', 0.0),\n",
       " ('guess_passwd', 144.0),\n",
       " ('portsweep', 1991911.0),\n",
       " ('ftp_write', 259.0),\n",
       " ('multihop', 1288.0),\n",
       " ('warezmaster', 301.0),\n",
       " ('warezclient', 627563.0),\n",
       " ('smurf', 0.0),\n",
       " ('normal', 21075991.0),\n",
       " ('satan', 64.0),\n",
       " ('ipsweep', 43.0),\n",
       " ('buffer_overflow', 2751.0),\n",
       " ('perl', 124.0),\n",
       " ('pod', 0.0),\n",
       " ('teardrop', 0.0),\n",
       " ('land', 0.0),\n",
       " ('rootkit', 1008.0),\n",
       " ('phf', 18.0),\n",
       " ('back', 284.0),\n",
       " ('imap', 72.0),\n",
       " ('nmap', 0.0),\n",
       " ('spy', 636.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_attack_by_key.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal', 21075991.0),\n",
       " ('portsweep', 1991911.0),\n",
       " ('warezclient', 627563.0),\n",
       " ('buffer_overflow', 2751.0),\n",
       " ('multihop', 1288.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_attack_by_key_sorted = duration_attack_by_key.sortBy(lambda x: x[1], ascending=False)\n",
    "duration_attack_by_key_sorted.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: There is a bunch of ways to compute the same thing ... We could have used functions like **keyBy, mapValues** ... refer to the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html) to have an overview !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "Debugging can be tedious in Spark. To help understanding the behaviour of the code we can print the logical plan created by Spark when doing operations.\n",
    "To have a comprehensive debug, it is good to use the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[34] at RDD at PythonRDD.scala:55 []\\n |  ParallelCollectionRDD[33] at parallelize at PythonRDD.scala:198 []'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sc = spark.sparkContext\n",
    "a = sc.parallelize([1,2,3])\n",
    "b = a.map(lambda x : x**2)\n",
    "c= b.filter(lambda x: x< 3)\n",
    "print(c.toDebugString())\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
