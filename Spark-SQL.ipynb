{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "   <img src=\"spark-sql.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Hands-on: Spark SQL\n",
    "\n",
    "In this notebook, we will focus on the SQL API of Spark. This API introduces the concept of Spark DataFrames (similar to pandas DataFrames) which are easier to manipulate. \n",
    "This is also the recommended API by Spark since it benefits from some internal optimization by the Spark engine. Spark SQL allows to manipulate structured data which RDD do not (but RDD is useful for unstructured data !).\n",
    "\n",
    "*Nota*: DataFrame is a structured object but is not typed. In scala (the language that develops Spark) you will find the Dataset datatype which is typed. In scala a DataFrame is simply a Dataset of \"Row\" type (e.g. val dataset : Dataset[Row] = spark.read ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import useful libs and load a Spark Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Foundry\n",
    "import foundrywrapper\n",
    "from foundrywrapper import FoundryWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>\n",
       "<b>Foundry-Wrapper</b> v0.3.3</br>\n",
       "Welcome JEREMY PIRARD  (PIRARD_J)<br>\n",
       "Using <a href='https://core.skywise.com/'>core.skywise.com</a>\n",
       "</p>"
      ],
      "text/plain": [
       "Foundry-Wrapper v0.3.3\n",
       "Welcome JEREMY PIRARD (PIRARD_J)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Required for foundry, not useful for spark purposes\n",
    "fw = FoundryWrapper()\n",
    "fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder \n",
    "        .master('spark://spark-master:7077') #master node URL\n",
    "        .appName('~ Spark hands-on: SQL~') #my App name\n",
    "        .config('spark.driver.memory', '1g') # memory allocated to the master\n",
    "        .config('spark.driver.cores', '2') # CPU's allocated to the master\n",
    "        .config('spark.executor.instances', '2') # how many executors\n",
    "        .config('spark.executor.memory', '4g') # memory per executor (where the data is stored)\n",
    "        .config('spark.executor.cores', '2') #CPU's per executor\n",
    "         .config('spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT',1)\\\n",
    "        .config('spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT',1) \\\n",
    "        .config(conf=fw.spark.get_spark_app_config())\n",
    "        .getOrCreate())\n",
    "# Configuring spark to use Foundry\n",
    "spark = fw.configure_spark(spark) # foundry specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.196.78.114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-palantir.65</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>~ Spark hands-on: SQL~</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4b2ea76ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load a dataset\n",
    "\n",
    "We will use the same dataset as in the RDD notebook, that is the Flight radar 24 dataset.\n",
    "Dataframe can be inferred from parquet, hive, csv, a rdd or even from a pandas df !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing dataset stats\n",
    "# Foundry wrapper\n",
    "file_rid = fw.compass.get_rid('/Shared/Airbus - FlightRadar24 (2014-2018)/data/flight_radar_24_week')\n",
    "# fw.catalog.get_dataset_view_stats(file_rid)\n",
    "df = spark.read.parquet(fw.spark.foundryfs_uri(file_rid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 ms, sys: 2.79 ms, total: 4.11 ms\n",
      "Wall time: 3.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "658975"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this data is structured and follow a given schema that we can access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flight_id: long (nullable = true)\n",
      " |-- aircraft_id: integer (nullable = true)\n",
      " |-- aircraft_registration: string (nullable = true)\n",
      " |-- equipment: string (nullable = true)\n",
      " |-- departure_airport_code: string (nullable = true)\n",
      " |-- scheduled_arrival_airport_code: string (nullable = true)\n",
      " |-- arrival_airport_code: string (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- callsign: string (nullable = true)\n",
      " |-- msn: integer (nullable = true)\n",
      " |-- elapsed_time_seconds: long (nullable = true)\n",
      " |-- departure_date_time: timestamp (nullable = true)\n",
      " |-- arrival_date_time: timestamp (nullable = true)\n",
      " |-- airport_separation: double (nullable = true)\n",
      " |-- track_distance: double (nullable = true)\n",
      " |-- adsb_start_flight_phase: string (nullable = true)\n",
      " |-- adsb_end_flight_phase: string (nullable = true)\n",
      " |-- out_time: timestamp (nullable = true)\n",
      " |-- off_time: timestamp (nullable = true)\n",
      " |-- on_time: timestamp (nullable = true)\n",
      " |-- in_time: timestamp (nullable = true)\n",
      " |-- OOOI_quality_rating: string (nullable = true)\n",
      " |-- next_flight_id: long (nullable = true)\n",
      " |-- next_arrival_airport_code: string (nullable = true)\n",
      " |-- turnaround_time: long (nullable = true)\n",
      " |-- dist_between_in_and_next_out: double (nullable = true)\n",
      " |-- trt_quality_rating: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- aircraft_family: string (nullable = true)\n",
      " |-- aircraft_type: string (nullable = true)\n",
      " |-- airline_iata: string (nullable = true)\n",
      " |-- airline_icao: string (nullable = true)\n",
      " |-- turnaround_time_minutes: decimal(38,18) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #helps visualizing what's inside a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: One could create a dataframe from a rdd by specifying a schema\n",
    "\n",
    "  See **createDataFrame** function and **StructField** (to create schema). https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#creating-dataframes\n",
    "  Here is an example to create a DataFrame out of a RDD by specifying the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|       R|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "sc = spark.sparkContext\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"R\", 20000), (\"Python\", 100000), (\"Scala\",3000)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "schema_field = StructType([StructField(columns[0], StringType(), True), StructField(columns[1], IntegerType(), True)])\n",
    "\n",
    "rdd_to_df = spark.createDataFrame(rdd, schema_field)\n",
    "rdd_to_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_to_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic cleaning of rows\n",
    "\n",
    "This API comes with a lot of built-in functions that allows to easily clean an input dataframe.\n",
    "For instance you could drop the duplicated rows or the rows containing NA's ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3764518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Select, filter, groupBy ...\n",
    "\n",
    "Basically, we can compute nearly all the functions that applies for a RDD. The structure of a DataFrame allows to do this in a fancy way. Recall that under the hood it will be internally transform to a RDD and still work with the transformations/actions and lazy computing ! ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this data engineering steps:\n",
    "- Select the following variables: MSN, elapsed_time, departure_airport_code and arrival_airport code.\n",
    "- Create a new binary variable if the flight duration is greater than a defined value\n",
    "- Filter only on long flights\n",
    "- Group the flights by their MSN\n",
    "- Compute their mean flight duration and the number of flights\n",
    "- Sort the MSN by their mean\n",
    "\n",
    "To do this we will use the spark sql functions. There are plenty of functions available. For this execerice we will need **select**, **withColumn** (creates a new variable conditionally to the value on another one), **when**, **otherwise** (used jointly with **withColumn**), **filter**, **groubBy**, **count** and **sort**.\n",
    "\n",
    "See [pyspark sql documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)  for a comprehensive list of module useful for manipulating DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------+\n",
      "| msn|mean_flight_duration|number_of_flights|\n",
      "+----+--------------------+-----------------+\n",
      "| 133|   53875.71428571428|                7|\n",
      "| 136|  52846.142857142855|                7|\n",
      "| 142|             52765.5|                8|\n",
      "| 147|             52740.6|                5|\n",
      "|  39|             48518.0|                6|\n",
      "|  35|            47651.75|                8|\n",
      "|  75|  46011.666666666664|                9|\n",
      "| 474|             45938.3|               10|\n",
      "|1468|   45644.77777777778|                9|\n",
      "| 395|             45313.0|                2|\n",
      "|  74|           45143.875|                8|\n",
      "| 718|             44583.5|                8|\n",
      "| 130|             44367.3|               10|\n",
      "|  22|           44268.625|                8|\n",
      "| 221|   44189.88888888889|                9|\n",
      "|  38|            44188.75|                8|\n",
      "| 151|           43990.125|                8|\n",
      "|  96|   43805.63636363636|               11|\n",
      "| 540|             43342.0|                4|\n",
      "| 296|             43127.0|                2|\n",
      "+----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.1 ms, sys: 4.96 ms, total: 14.1 ms\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_selected = df.select(df[\"msn\"], df[\"elapsed_time_seconds\"], df[\"departure_airport_code\"], df[\"arrival_airport_code\"]) #transformation\n",
    "df_with_binary_var = df_selected.withColumn(\"is_long_flight\", F.when(df_selected[\"elapsed_time_seconds\"] > 5000,1).otherwise(0)) #transformation\n",
    "df_filtered = df_with_binary_var.filter(df_with_binary_var[\"is_long_flight\"] == 1) #transformation\n",
    "df_grouped = df_filtered.groupBy(\"msn\").agg(F.mean(\"elapsed_time_seconds\").alias(\"mean_flight_duration\"), F.count(\"msn\").alias(\"number_of_flights\")) # I have a doubt with the aggregation here !\n",
    "df_sorted = df_grouped.sort(df_grouped[\"mean_flight_duration\"], ascending=False) #transformation\n",
    "df_sorted.show() # the action that will trigger the computations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: To see what happens under the hood, you can print the logical execution plan of Spark. Sometimes the optimization engine will change the order of your computation to minimize computation time ! This also allow to check for useless shuffling or error before calling an action that could lead to an unwanted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [mean_flight_duration#256 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(mean_flight_duration#256 DESC NULLS LAST, 200), true\n",
      "   +- *(2) HashAggregate(keys=[msn#145], functions=[avg(elapsed_time_seconds#146L), count(msn#145)])\n",
      "      +- Exchange hashpartitioning(msn#145, 200), true\n",
      "         +- *(1) HashAggregate(keys=[msn#145], functions=[partial_avg(elapsed_time_seconds#146L), partial_count(msn#145)])\n",
      "            +- *(1) Filter (CASE WHEN (elapsed_time_seconds#146L > 5000) THEN 1 ELSE 0 END = 1)\n",
      "               +- *(1) FileScan parquet [msn#145,elapsed_time_seconds#146L] Batched: true, DataFilters: [(CASE WHEN (elapsed_time_seconds#146L > 5000) THEN 1 ELSE 0 END = 1)], Format: Parquet, Location: InMemoryFileIndex[foundry://sectokenjhub/datasets/ri.foundry.main.dataset.668c21bc-0a7a-4ad7-9cb2..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<msn:int,elapsed_time_seconds:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted.explain() #this is done like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have built it in a single pipeline for better readibility:\n",
    "\n",
    "*Nota*: We can access a column using df[\"colname\"] or by F.col(\"colname\")!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------+\n",
      "| msn|mean_flight_duration|number_of_flights|\n",
      "+----+--------------------+-----------------+\n",
      "| 133|   53875.71428571428|                7|\n",
      "| 136|  52846.142857142855|                7|\n",
      "| 142|             52765.5|                8|\n",
      "| 147|             52740.6|                5|\n",
      "|  39|             48518.0|                6|\n",
      "|  35|            47651.75|                8|\n",
      "|  75|  46011.666666666664|                9|\n",
      "| 474|             45938.3|               10|\n",
      "|1468|   45644.77777777778|                9|\n",
      "| 395|             45313.0|                2|\n",
      "|  74|           45143.875|                8|\n",
      "| 718|             44583.5|                8|\n",
      "| 130|             44367.3|               10|\n",
      "|  22|           44268.625|                8|\n",
      "| 221|   44189.88888888889|                9|\n",
      "|  38|            44188.75|                8|\n",
      "| 151|           43990.125|                8|\n",
      "|  96|   43805.63636363636|               11|\n",
      "| 540|             43342.0|                4|\n",
      "| 296|             43127.0|                2|\n",
      "+----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 10.6 ms, sys: 5.18 ms, total: 15.8 ms\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df[\"is_long_flight\"] raises an error at the filtering step. that is weird and probably due to the fact that it does not know that we created a new column before during the pipelining\n",
    "df_sorted = df.select(df[\"msn\"], df[\"elapsed_time_seconds\"], df[\"departure_airport_code\"], df[\"arrival_airport_code\"]) \\\n",
    "  .withColumn(\"is_long_flight\", F.when(df[\"elapsed_time_seconds\"] > 5000,1).otherwise(0)) \\\n",
    "  .filter((F.col(\"is_long_flight\") == 1)) \\\n",
    "  .groupBy(\"msn\").agg(F.mean(\"elapsed_time_seconds\").alias(\"mean_flight_duration\"), F.count(\"msn\").alias(\"number_of_flights\")) \\\n",
    "  .sort(F.col(\"mean_flight_duration\"), ascending=False)\n",
    "\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. In a SQL way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data wrangling can be done using a friendly SQL typing which allow to re-use knowledge from SQL typing coupled with the power of Spark. \n",
    "To do this, you have to register the dataframe used into a SQL table using **createOrReplaceTempView** function. The outpout of a SQL query is also a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"flight_radar\") #cf createOrReplaceGlobalView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_sql= spark.sql(\"\"\" SELECT msn, COUNT(msn) AS number_of_flights, AVG(elapsed_time_seconds) AS mean_flight_duration\n",
    "FROM flight_radar\n",
    "WHERE  elapsed_time_seconds > 5000\n",
    "GROUP BY msn\n",
    "ORDER BY mean_flight_duration DESC\n",
    "LIMIT 20\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------------------+\n",
      "| msn|number_of_flights|mean_flight_duration|\n",
      "+----+-----------------+--------------------+\n",
      "| 133|                7|   53875.71428571428|\n",
      "| 136|                7|  52846.142857142855|\n",
      "| 142|                8|             52765.5|\n",
      "| 147|                5|             52740.6|\n",
      "|  39|                6|             48518.0|\n",
      "|  35|                8|            47651.75|\n",
      "|  75|                9|  46011.666666666664|\n",
      "| 474|               10|             45938.3|\n",
      "|1468|                9|   45644.77777777778|\n",
      "| 395|                2|             45313.0|\n",
      "|  74|                8|           45143.875|\n",
      "| 718|                8|             44583.5|\n",
      "| 130|               10|             44367.3|\n",
      "|  22|                8|           44268.625|\n",
      "| 221|                9|   44189.88888888889|\n",
      "|  38|                8|            44188.75|\n",
      "| 151|                8|           43990.125|\n",
      "|  96|               11|   43805.63636363636|\n",
      "| 540|                4|             43342.0|\n",
      "| 296|                2|             43127.0|\n",
      "+----+-----------------+--------------------+\n",
      "\n",
      "CPU times: user 3.54 ms, sys: 665 µs, total: 4.21 ms\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sorted_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Windowing\n",
    "\n",
    "Let's say that out of the preceding DataFrame you want to compute the absolute difference of the elapsed_time_flight with the mean computed by the groupBy. \n",
    "Since it is grouped, you could not concatenate the column containing the mean with the initial DataFrame. To do that with usual spark functions, you would have to do a **join**.\n",
    "\n",
    "**Window** allows you to do that in a more efficient way. Note that they are alos very useful for computing difference with an aggregating measure or for instance doing rolling statistics (rolling mean, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+\n",
      "|msn|elapsed_time_seconds| mean_elapsed_time|\n",
      "+---+--------------------+------------------+\n",
      "|148|                 404|18720.560975609755|\n",
      "|148|               37456|18720.560975609755|\n",
      "|148|               43212|18720.560975609755|\n",
      "|148|               16864|18720.560975609755|\n",
      "|148|               36606|18720.560975609755|\n",
      "+---+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "#We defined the window and partition the data by msn. Otherwise all the data would be retrieved on a single machine !\n",
    "window = Window \\\n",
    ".partitionBy(df[\"msn\"]) \n",
    "\n",
    "windowed_mean_df = df.select(df[\"msn\"], df[\"elapsed_time_seconds\"]) \\\n",
    ".withColumn(\"mean_elapsed_time\", F.avg(df[\"elapsed_time_seconds\"]).over(window))\n",
    "\n",
    "windowed_mean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This replicates the mean value over all rows. Now it is easy to compute the difference !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-------------------------------+\n",
      "|msn|elapsed_time_seconds| mean_elapsed_time|abs_diff_with_mean_elapsed_time|\n",
      "+---+--------------------+------------------+-------------------------------+\n",
      "|148|                 404|18720.560975609755|             18316.560975609755|\n",
      "|148|               37456|18720.560975609755|             18735.439024390245|\n",
      "|148|               43212|18720.560975609755|             24491.439024390245|\n",
      "|148|               16864|18720.560975609755|             1856.5609756097547|\n",
      "|148|               36606|18720.560975609755|             17885.439024390245|\n",
      "|148|               21671|18720.560975609755|             2950.4390243902453|\n",
      "|148|                 615|18720.560975609755|             18105.560975609755|\n",
      "|148|                   0|18720.560975609755|             18720.560975609755|\n",
      "|148|               47011|18720.560975609755|             28290.439024390245|\n",
      "|148|               36602|18720.560975609755|             17881.439024390245|\n",
      "|148|                9748|18720.560975609755|              8972.560975609755|\n",
      "|148|                1841|18720.560975609755|             16879.560975609755|\n",
      "|148|               38817|18720.560975609755|             20096.439024390245|\n",
      "|148|               42807|18720.560975609755|             24086.439024390245|\n",
      "|148|                 136|18720.560975609755|             18584.560975609755|\n",
      "|148|               41534|18720.560975609755|             22813.439024390245|\n",
      "|148|                 359|18720.560975609755|             18361.560975609755|\n",
      "|148|               38685|18720.560975609755|             19964.439024390245|\n",
      "|148|               16024|18720.560975609755|             2696.5609756097547|\n",
      "|148|               42375|18720.560975609755|             23654.439024390245|\n",
      "+---+--------------------+------------------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2.33 ms, sys: 3.37 ms, total: 5.7 ms\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "diff_mean_df = windowed_mean_df.withColumn(\"abs_diff_with_mean_elapsed_time\", F.abs(windowed_mean_df[\"elapsed_time_seconds\"] - windowed_mean_df[\"mean_elapsed_time\"])) \n",
    "diff_mean_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window are very useful to compute rolling statistics or difference. Let's compute the difference between every elapsed_time flight for each msn. To do that we need to group the data per msn, order them by the departure date and then compute the difference between one flight and the next one.\n",
    "We will use the **orderBy** function from window and **lag** SQL function which shifts a column by a fixed step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+-------------------+\n",
      "|msn|elapsed_time_seconds|departure_date_time|lagged_elapsed_time|\n",
      "+---+--------------------+-------------------+-------------------+\n",
      "|148|                 404|2015-01-01 04:39:52|               null|\n",
      "|148|                5687|2015-01-01 06:01:15|                404|\n",
      "|148|               37456|2015-01-01 06:26:31|               5687|\n",
      "|148|                5835|2015-01-01 10:16:05|              37456|\n",
      "|148|               30392|2015-01-01 14:54:06|               5835|\n",
      "|148|               43212|2015-01-01 17:31:48|              30392|\n",
      "|148|               24875|2015-01-02 02:38:05|              43212|\n",
      "|148|               16864|2015-01-02 05:32:50|              24875|\n",
      "|148|               36606|2015-01-02 10:15:44|              16864|\n",
      "|148|               13341|2015-01-02 11:24:12|              36606|\n",
      "|148|               13112|2015-01-02 23:36:02|              13341|\n",
      "|148|               21671|2015-01-02 23:54:27|              13112|\n",
      "|148|                 615|2015-01-03 05:55:53|              21671|\n",
      "|148|                5172|2015-01-03 06:01:35|                615|\n",
      "|148|                   0|2015-01-03 08:21:48|               5172|\n",
      "|148|               47011|2015-01-03 08:30:28|                  0|\n",
      "|148|                5493|2015-01-03 14:47:38|              47011|\n",
      "|148|               23951|2015-01-03 17:44:58|               5493|\n",
      "|148|               36602|2015-01-04 00:43:07|              23951|\n",
      "|148|               23903|2015-01-04 02:12:52|              36602|\n",
      "+---+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_df = df.select(df[\"msn\"], df[\"elapsed_time_seconds\"], df[\"departure_date_time\"]) \\\n",
    ".withColumn(\"lagged_elapsed_time\", F.lag(df[\"elapsed_time_seconds\"]).over(Window.partitionBy(\"msn\").orderBy(\"departure_date_time\")))\n",
    "\n",
    "lagged_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota*: We get a null value since we cannot shift the first row !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+-------------------+-----------------+\n",
      "|msn|elapsed_time_seconds|departure_date_time|lagged_elapsed_time|diff_elapsed_time|\n",
      "+---+--------------------+-------------------+-------------------+-----------------+\n",
      "|148|                 404|2015-01-01 04:39:52|               null|             null|\n",
      "|148|                5687|2015-01-01 06:01:15|                404|             5283|\n",
      "|148|               37456|2015-01-01 06:26:31|               5687|            31769|\n",
      "|148|                5835|2015-01-01 10:16:05|              37456|           -31621|\n",
      "|148|               30392|2015-01-01 14:54:06|               5835|            24557|\n",
      "|148|               43212|2015-01-01 17:31:48|              30392|            12820|\n",
      "|148|               24875|2015-01-02 02:38:05|              43212|           -18337|\n",
      "|148|               16864|2015-01-02 05:32:50|              24875|            -8011|\n",
      "|148|               36606|2015-01-02 10:15:44|              16864|            19742|\n",
      "|148|               13341|2015-01-02 11:24:12|              36606|           -23265|\n",
      "+---+--------------------+-------------------+-------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_df = lagged_df.withColumn(\"diff_elapsed_time\", lagged_df[\"elapsed_time_seconds\"] - lagged_df[\"lagged_elapsed_time\"])\n",
    "diff_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. User-Defined functions (UDF) and pandas UDF\n",
    "\n",
    "Sometimes you do want to create a new column by applying a custom function that you design and is not covered with the native spark functions. It looks like we would like to do a **map** operation like we did on RDD. However this is not a SQL function and you have to go back to RDD to apply a map function.\n",
    "User-defined functions are here to help us avoiding this matter. They are simple python function that are wrapped into a **udf** (or **pandas_udf**) and will be assigned to a whole column.\n",
    "\n",
    "First you have to define a python function that applies to one element. Then you register it to an UDF because UDF applies on column data type. It is then easy to add it using a **withColumn** statement.\n",
    "\n",
    "*Nota*: **pandas_udf** treats the column as a Pandas Series. It is recommanded to use it for a good interaction with **pandas** python library and Apache Arrow for optimizations.\n",
    "\n",
    "*Nota2*: Always look at the documentation to see if the function you want might already exist. UDF are conveniant but are not efficient due to serialization issues. pandas_udf using apache arrow does better but still use a lot of communication between JVM and python process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a toy example, let's say that we would like to tag the short and long flight according to the *elapsed_time_duration* variable. We will define an User-Defined function to do this.\n",
    "\n",
    "*Question*: How would you do this with built-in spark function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function as you would do in classic python\n",
    "def is_long_flight(f):\n",
    "    return \"short\" if f < 5000 else \"long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediary step: Register the function as an UDF\n",
    "\n",
    "is_long_flight_UDF = udf(is_long_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "| msn|elapsed_time_seconds|is_long_flight|\n",
      "+----+--------------------+--------------+\n",
      "|7561|               15433|          long|\n",
      "| 103|               20102|          long|\n",
      "|  46|               26893|          long|\n",
      "|  40|               23682|          long|\n",
      "| 139|               24688|          long|\n",
      "|  90|               25582|          long|\n",
      "| 164|               23870|          long|\n",
      "|  23|               23804|          long|\n",
      "|  89|                 438|         short|\n",
      "| 138|               54886|          long|\n",
      "|  31|               31949|          long|\n",
      "|  55|               32175|          long|\n",
      "|  65|               25322|          long|\n",
      "|  34|               20434|          long|\n",
      "| 119|                  80|         short|\n",
      "|  96|               45562|          long|\n",
      "|  12|               28335|          long|\n",
      "|  79|               49171|          long|\n",
      "|  11|                  10|         short|\n",
      "|  82|               16760|          long|\n",
      "|  68|               45714|          long|\n",
      "|  63|               53206|          long|\n",
      "|  71|               21452|          long|\n",
      "| 122|               19050|          long|\n",
      "| 131|                7637|          long|\n",
      "|  21|               12809|          long|\n",
      "|  87|               21189|          long|\n",
      "| 147|               48759|          long|\n",
      "| 121|                   0|         short|\n",
      "|  54|                   0|         short|\n",
      "|  36|                 890|         short|\n",
      "| 169|               43813|          long|\n",
      "|  54|                   0|         short|\n",
      "| 121|                   0|         short|\n",
      "| 115|               27281|          long|\n",
      "| 109|               22084|          long|\n",
      "| 121|                   0|         short|\n",
      "|  54|                  20|         short|\n",
      "|  36|                   0|         short|\n",
      "|  94|               49099|          long|\n",
      "|  54|                 100|         short|\n",
      "|  25|                7149|          long|\n",
      "|  36|                1392|         short|\n",
      "|   7|                7702|          long|\n",
      "| 131|                 116|         short|\n",
      "| 112|               27542|          long|\n",
      "|  17|               42430|          long|\n",
      "|  80|               25198|          long|\n",
      "| 131|               14508|          long|\n",
      "| 120|                 631|         short|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column with the outputs of is_long_flight\n",
    "\n",
    "df.withColumn(\"is_long_flight\", is_long_flight_UDF(df[\"elapsed_time_seconds\"])).select(\"msn\", \"elapsed_time_seconds\",\"is_long_flight\") \\\n",
    "  .where(df[\"msn\"].isNotNull()).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with **pandas_udf**. Let's use a pandas_udf to compute the cumulative sum of the elapsed_time. This allows to use the *pandas* object types (*Series*) and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cum_sum(x):\n",
    "    return x.cumsum() #Series type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the function as a pandas_udf\n",
    "\n",
    "cum_sum_pandas_UDF = pandas_udf(cum_sum, returnType=LongType()) #needs to provide the return type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|cum_sum(elapsed_time_seconds)|\n",
      "+-----------------------------+\n",
      "|                          151|\n",
      "|                         2303|\n",
      "|                         4078|\n",
      "|                         6144|\n",
      "|                         7971|\n",
      "|                        15456|\n",
      "|                        24409|\n",
      "|                        25404|\n",
      "|                        25624|\n",
      "|                        26634|\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(cum_sum_pandas_UDF(df[\"elapsed_time_seconds\"])).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Some words about **caching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a DataFrame is used accros many queries or with an iterative algorithm, it requires to read it many times which can be painful in termns of computational time, network cost (shuffling) ...\n",
    "Spark supports pulling datasets into a cluster-wide memory cache. It is good practice to cache what is used repeatedly or hard to compute. Caching is lazy, so you pay the cost to when providing the very first action. But then it might makes a huge gain of performance.\n",
    "\n",
    "e.g.\n",
    "\n",
    "````python\n",
    "data = loading_data(...).cache()\n",
    "one_use_case = data.groupBy(...).agg(...).show()\n",
    "another_use_case = data.groupBy(...).agg(...).show()\n",
    "````\n",
    "\n",
    "To cache a dataset into memory, juste use the **cache** or **persist** function. **persist** gives you the possibility to store you data either on memory or on disk ...\n",
    "Don't forget to free the memory of the cluster when the dataset is not needed anymore (using **unpersist**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
